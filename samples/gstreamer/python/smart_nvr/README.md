# Smart Network Video Recorder for Line Hogging

This sample illustrates how to build a simple NVR recorder with user-defined video analytic logic.
The sample detects line hogging events, which may be illegal in certain jurisdictions.

The sample focuses on demonstrating how to integrate custom logic into a DLStreamer pipeline. 
The actual line hogging detection logic is relatively simple and is used only for demonstration purposes.

The individual pipeline stages implement the following functions:

* __filesrc__ element reads the video stream from a local file
* __decodebin3__ element decodes the video stream into individual frames
* __gvadetect__ element runs an open-vocabulary AI detection model on each frame
* __gvaanalytics_py__ is a custom (Python) element that examines detection metadata to detect line hogging objects
* __gvarecorder_py__ is a custom (Python) element that splits the video stream into 10-second chunks and appends metadata with a list of objects detected in each chunk

## How It Works

### STEP 1 - Model Download and Conversion

First, the sample downloads an example video file and the RTDETRv2 object detection model from HuggingFace. 
The RTDETR PyTorch model is converted to OpenVINO IR (with ONNX as an intermediate step) using the standard HuggingFace toolchain.
Additionally, the sample downloads 'preprocessor_config.json' along with the model file. 
DLStreamer inference elements parse this configuration file to set up image preprocessing steps.

```code
subprocess.run(["optimum-cli", "export", "onnx", "--model", "PekingU/rtdetr_v2_r50vd", 
                "--task", "object-detection", "--opset", "18", "--width", "640", "--height", "640", "rtdetr_v2_r50vd"],
    check=True)
subprocess.run(["hf", "download", "PekingU/rtdetr_v2_r50vd", "--include", "preprocessor_config.json", "--local-dir", "."], check=True)
subprocess.run(["ovc", "model.onnx"], check=True)
```

Note that the sample checks if the video file and model are already downloaded and skips this step in subsequent runs.

### STEP 2 - DLStreamer Pipeline Construction

The application creates a GStreamer `pipeline` composed of predefined GStreamer/DLStreamer elements and custom Python elements developed as part of this sample.

```code
pipeline = Gst.parse_launch(
    f"filesrc location={video_file} ! decodebin3 ! "
    f"gvadetect model={ov_model_path} device=GPU batch-size=4 threshold=0.7 ! queue ! "
    f"gvaanalytics_py distance=500 angle=-135,-45 ! gvawatermark ! "
    f"gvarecorder_py location=output.mp4 max-time=10")
pipeline_loop(pipeline)
```

### STEP 3 - Custom Analytics Element

The `gvaanalytics_py` element is defined in plugins/python/gvaAnalytics.py.

This transform element reads GstAnalytics metadata generated by gvadetect and adds new metadata elements. 
The custom element implements the following logic:
- Search for car or truck objects crossing outer lanes (as defined by the 'zone' polygon)
- If a car or truck crosses the zone, check whether there is a neighboring car driving in the left lane (as defined by 'distance' and 'angle')
- If no neighboring object is detected, classify the inspected car as line hogging and insert a new "hogging" object

### STEP 4 - Custom Video File Storage Element

The `gvarecorder_py` element is defined in plugins/python/gvaRecorder.py.

It is a GStreamer bin element that uses existing GStreamer elements to encode the video stream and split it into N-second video segments. 
The element registers custom callbacks and signal handlers.

```code
self.get_static_pad("sink").add_probe(Gst.PadProbeType.BUFFER, self.buffer_probe, 0)
self.get_static_pad("sink").add_probe(Gst.PadProbeType.EVENT_DOWNSTREAM, self.event_probe, 0)
self._filesink.connect("format-location", self.format_location_callback, 0)
```

The 'buffer_probe' callback accumulates object categories detected by preceding elements ('gvadetect' and 'gvaanalytics_py').

The 'format_location_callback' is called whenever a new video chunk starts. 
This callback stores collected metadata in a file.

## Running

The sample application does not require any assets; it downloads the video file and model from within the sample code.
However, the application requires additional Python packages to run:

```sh
pip install -r requirements.txt
python3 ./smart_nvr.py 1192116-sd_640_360_30fps.mp4 "white car"
```

The sample outputs detection results to files stored in the current directory:
```sh
output-00.txt
output-00.mp4
output-01.txt
output-01.mp4
...
```

The output metadata file lists objects detected in each video chunk:
```sh
Objects: ['car', 'hogging', 'truck']
```

The metadata files can be quickly searched for 'hogging' objects, and you can inspect the corresponding video file chunk to see the careless driving car or truck.

## See also
* [Samples overview](../../README.md)
